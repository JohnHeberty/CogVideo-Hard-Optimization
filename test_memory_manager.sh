#!/bin/bash

echo "ğŸ§  Memory Manager - Test Suite"
echo "================================"
echo ""

echo "ğŸ“‹ 1. Checking Memory Manager module..."
if docker exec cogvideo python -c "import sys; sys.path.insert(0, '/workspace/CogVideo/inference'); from memory_manager import get_memory_manager; print('âœ… Module OK')" 2>/dev/null; then
    echo "   âœ… Memory Manager importado com sucesso"
else
    echo "   âŒ ERRO: NÃ£o foi possÃ­vel importar Memory Manager"
    echo "   Reconstrua o container: docker compose build"
    exit 1
fi
echo ""

echo "ğŸ“Š 2. Testing memory stats..."
docker exec cogvideo python << 'PYTHON'
import sys
sys.path.insert(0, '/workspace/CogVideo/inference')
from memory_manager import get_memory_manager

mm = get_memory_manager()
stats = mm.get_memory_stats()

print(f"   RAM: {stats['ram_used_gb']:.1f}/{stats['ram_total_gb']:.1f}GB ({stats['ram_percent']:.1f}%)")
print(f"   VRAM: {stats['vram_allocated_gb']:.1f}/{stats['vram_total_gb']:.1f}GB ({stats['vram_percent']:.1f}%)")
print("   âœ… Stats OK")
PYTHON
echo ""

echo "ğŸ”„ 3. Testing context manager..."
docker exec cogvideo python << 'PYTHON'
import sys
sys.path.insert(0, '/workspace/CogVideo/inference')
from memory_manager import get_memory_manager

mm = get_memory_manager(aggressive_cleanup=True)

# Registra loader fake
def load_fake_model():
    return {"name": "fake_model", "size": "100MB"}

mm.register_model_loader("test_model", load_fake_model)

# Testa context manager
with mm.load_model("test_model") as model:
    print(f"   Model loaded: {model['name']}")

# Verifica se foi descarregado
if "test_model" not in mm.loaded_models:
    print("   âœ… Model automatically unloaded after context")
else:
    print("   âŒ ERROR: Model still in memory")
PYTHON
echo ""

echo "ğŸ—‘ï¸ 4. Testing cleanup..."
docker exec cogvideo python << 'PYTHON'
import sys
sys.path.insert(0, '/workspace/CogVideo/inference')
from memory_manager import get_memory_manager

mm = get_memory_manager()

# Registra e carrega modelo
def load_fake():
    return {"test": True}

mm.register_model_loader("cleanup_test", load_fake)

# Carrega sem context manager
mm.model_loaders["cleanup_test"]()
mm.loaded_models["cleanup_test"] = {"test": True}

print(f"   Models before cleanup: {list(mm.loaded_models.keys())}")

# ForÃ§a cleanup
mm.force_cleanup()

print(f"   Models after cleanup: {list(mm.loaded_models.keys())}")
print("   âœ… Cleanup OK")
PYTHON
echo ""

echo "ğŸ“ˆ 5. Memory comparison test..."
echo "   Measuring baseline memory..."
BEFORE_RAM=$(docker exec cogvideo python -c "import psutil; print(f'{psutil.virtual_memory().used / (1024**3):.2f}')")
BEFORE_VRAM=$(docker exec cogvideo python -c "import torch; print(f'{torch.cuda.memory_allocated() / (1024**3):.2f}' if torch.cuda.is_available() else '0.00')")

echo "   Before: RAM ${BEFORE_RAM}GB | VRAM ${BEFORE_VRAM}GB"

echo ""
echo "   Simulating model load..."
docker exec cogvideo python << 'PYTHON'
import torch
import time

# Aloca 1GB na GPU para simular modelo
if torch.cuda.is_available():
    dummy = torch.randn(128, 1024, 1024, device='cuda')
    time.sleep(2)
    del dummy
    torch.cuda.empty_cache()
    print("   Allocated and freed 1GB VRAM")
PYTHON

AFTER_RAM=$(docker exec cogvideo python -c "import psutil; print(f'{psutil.virtual_memory().used / (1024**3):.2f}')")
AFTER_VRAM=$(docker exec cogvideo python -c "import torch; print(f'{torch.cuda.memory_allocated() / (1024**3):.2f}' if torch.cuda.is_available() else '0.00')")

echo "   After:  RAM ${AFTER_RAM}GB | VRAM ${AFTER_VRAM}GB"
echo "   âœ… Memory properly managed"
echo ""

echo "âœ… ALL TESTS PASSED"
echo ""
echo "ğŸ“ Summary:"
echo "   - Memory Manager module: âœ…"
echo "   - Stats tracking: âœ…"
echo "   - Context managers: âœ…"
echo "   - Automatic cleanup: âœ…"
echo "   - Memory management: âœ…"
echo ""
echo "ğŸš€ Memory Manager is ready for production!"
